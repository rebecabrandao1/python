# -*- coding: utf-8 -*-
"""grafico.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uaqv69zVXR8MD1Sph6FZVTDzc4wy05Bh

**Regressão**

***Importando bibliotecas***
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pnd
import matplotlib.pyplot as plt
# %matplotlib inline

"""**Criando dataset**"""

x = [0.5,2.2,1.0,2.9,3.7]
 y = [2.8,1.4,5.0,2.7,3.2]

"""**taxa de aprendizagem e coeficientes(angular e linear)**"""

alpha = 0.01
 w0 = 3.2
 w1 = 2.1

"""**Função da hipótese**"""

def hipo(w0,w1,x):
  hipo = w0 + (w1 * x)
  return hipo

##realizando uma predição sem nada treinado

hipo(w0,w1,3.7)

"""**Função para a plotagem**"""

def plota(x,y,w0,w1):
  x_values = [i for i in range(int(min(x))-1,int(max(x))+2)]
  y_values = [hipo(x, w0, w1) for x in x_values ]
  plt.plot(x_values, y_values, 'r')
  plt.scatter(x,y, color='blue')

plota(x,y,w0,w1)

"""**Função de Erro/soma dos custos**"""

def sc(w0,w1,x,y):
  custo = 0
  m = float(len(x))
  for i in range (0,len(x)):
    custo = custo + (hipo(w0,w1,x[i])- y[i])**2
    return custo/m

## testando neah (função de erro/soma dos custos)

sc(w0,w1,x,y)

"""**Gradiente descendente**"""

def grad(w0,w1,x,y,alpha):
  gradw0 = 0
  gradw1 = 0
  m = float(len(x))
  for i in range (0,len(x)):
    gradw0 = (1/m) *  (hipo(w0,w1,x[i])- y[i])
    gradw1 = (1/m) *  (hipo(w0,w1,x[i])- y[i]) * x[i] 
    gradi = w0 - alpha * gradw0
    gradi1 = w1 - alpha * gradw1
    return gradi , gradi1

epocas = 10000
custo = np.zeros(epocas)
for i in range (epocas):
  w0 , w1 = grad(w0,w1,x,y,alpha)
  custo[i] = sc(w0,w1,x,y)

plota(x,y,w0,w1)

plotcusto = pnd.DataFrame(custo)
plotcusto.plot()

## testando a predição com modelo treinado

hipo(w0,w1,3.7)

